{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تحميل المكتبات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمل دالة المسافة لاستخدامها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist1(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "def dist2(a, b):\n",
    "    return 1 - a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "dist, metric = dist2, 'cosine'\n",
    "# dist, metric = dist1, 'euclidean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمل دالة لتنفيذ عملية الطرح"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3):\n",
    "    for w in (w1, w2, w3):\n",
    "        if w not in word2vec:\n",
    "            print(\"%s not in dictionary\" % w)\n",
    "            return\n",
    "\n",
    "    king = word2vec[w1]\n",
    "    man = word2vec[w2]\n",
    "    woman = word2vec[w3]\n",
    "    v0 = king - man + woman\n",
    "\n",
    "    min_dist = float('inf')\n",
    "    best_word = ''\n",
    "    for word, v1 in iteritems(word2vec):\n",
    "        if word not in (w1, w2, w3):\n",
    "            d = dist(v0, v1)\n",
    "            if d < min_dist:\n",
    "                min_dist = d\n",
    "                best_word = word\n",
    "    print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نفس الدالة السابقة بطريقة اسرع"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3):\n",
    "    for w in (w1, w2, w3):\n",
    "        if w not in word2vec:\n",
    "            print(\"%s not in dictionary\" % w)\n",
    "            return\n",
    "\n",
    "    king = word2vec[w1]\n",
    "    man = word2vec[w2]\n",
    "    woman = word2vec[w3]\n",
    "    v0 = king - man + woman\n",
    "\n",
    "    distances = pairwise_distances(v0.reshape(1, D),\n",
    "                                   embedding, metric=metric).reshape(V)\n",
    "    idxs = distances.argsort()[:4]\n",
    "    for idx in idxs:\n",
    "        word = idx2word[idx]\n",
    "        if word not in (w1, w2, w3): \n",
    "            best_word = word\n",
    "            break\n",
    "\n",
    "    print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمل دالة لاختيار الكلمات الاقرب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(w, n=5):\n",
    "    if w not in word2vec:\n",
    "        print(\"%s not in dictionary:\" % w)\n",
    "        return\n",
    "\n",
    "    v = word2vec[w]\n",
    "    distances = pairwise_distances(v.reshape(1, D),\n",
    "                                   embedding, metric=metric).reshape(V)\n",
    "    idxs = distances.argsort()[1:n+1]\n",
    "    print(\"neighbors of: %s\" % w)\n",
    "    for idx in idxs:\n",
    "        print(\"\\t%s\" % idx2word[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "قراءة البيانات "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تحميل الملف من هنا \n",
    "\n",
    "\n",
    "https://www.kaggle.com/datasets/watts2/glove6b50dtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "embedding = []\n",
    "idx2word = []\n",
    "\n",
    "with open(r'D:\\Machine Learning\\NLP\\0 Data\\GloVe English\\glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "    \n",
    " # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        embedding.append(vec)\n",
    "        idx2word.append(word)\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "embedding = np.array(embedding)\n",
    "V, D = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الاطلاع علي البيانات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word the   ,   with Vecs     [ 0.418       0.24968    -0.41242     0.1217      0.34527    -0.044457\n",
      " -0.49688    -0.17862    -0.00066023 -0.6566    ]\n",
      "word ,   ,   with Vecs     [ 0.013441  0.23682  -0.16899   0.40951   0.63812   0.47709  -0.42852\n",
      " -0.55641  -0.364    -0.23938 ]\n",
      "word .   ,   with Vecs     [ 0.15164  0.30177 -0.16763  0.17684  0.31719  0.33973 -0.43478 -0.31086\n",
      " -0.44999 -0.29486]\n",
      "word of   ,   with Vecs     [ 0.70853  0.57088 -0.4716   0.18048  0.54449  0.72603  0.18157 -0.52393\n",
      "  0.10381 -0.17566]\n",
      "word to   ,   with Vecs     [ 0.68047  -0.039263  0.30186  -0.17792   0.42962   0.032246 -0.41376\n",
      "  0.13228  -0.29847  -0.085253]\n",
      "word and   ,   with Vecs     [ 0.26818   0.14346  -0.27877   0.016257  0.11384   0.69923  -0.51332\n",
      " -0.47368  -0.33075  -0.13834 ]\n",
      "word in   ,   with Vecs     [ 0.33042   0.24995  -0.60874   0.10923   0.036372  0.151    -0.55083\n",
      " -0.074239 -0.092307 -0.32821 ]\n",
      "word a   ,   with Vecs     [ 0.21705  0.46515 -0.46757  0.10082  1.0135   0.74845 -0.53104 -0.26256\n",
      "  0.16812  0.13182]\n",
      "word \"   ,   with Vecs     [ 0.25769   0.45629  -0.76974  -0.37679   0.59272  -0.063527  0.20545\n",
      " -0.57385  -0.29009  -0.13662 ]\n",
      "word 's   ,   with Vecs     [ 0.23727  0.40478 -0.20547  0.58805  0.65533  0.32867 -0.81964 -0.23236\n",
      "  0.27428  0.24265]\n",
      "word for   ,   with Vecs     [ 0.15272   0.36181  -0.22168   0.066051  0.13029   0.37075  -0.75874\n",
      " -0.44722   0.22563   0.10208 ]\n",
      "word -   ,   with Vecs     [-0.16768  1.2151   0.49515  0.26836 -0.4585  -0.23311 -0.52822 -1.3557\n",
      "  0.16098  0.37691]\n",
      "word that   ,   with Vecs     [ 0.88387  -0.14199   0.13566   0.098682  0.51218   0.49138  -0.47155\n",
      " -0.30742   0.01963   0.12686 ]\n",
      "word on   ,   with Vecs     [ 0.30045   0.25006  -0.16692   0.1923    0.026921 -0.079486 -0.91383\n",
      " -0.1974   -0.053413 -0.40846 ]\n",
      "word is   ,   with Vecs     [ 0.6185     0.64254   -0.46552    0.3757     0.74838    0.53739\n",
      "  0.0022239 -0.60577    0.26408    0.11703  ]\n",
      "word was   ,   with Vecs     [ 0.086888 -0.19416  -0.24267  -0.33391   0.56731   0.39783  -0.97809\n",
      "  0.03159  -0.61469  -0.31406 ]\n",
      "word said   ,   with Vecs     [ 0.38973 -0.2121   0.51837  0.80136  1.0336  -0.27784 -0.84525 -0.25333\n",
      "  0.12586 -0.90342]\n",
      "word with   ,   with Vecs     [ 0.25616   0.43694  -0.11889   0.20345   0.41959   0.85863  -0.60344\n",
      " -0.31835  -0.6718    0.003984]\n",
      "word he   ,   with Vecs     [-0.20092  -0.060271 -0.61766  -0.8444    0.5781    0.14671  -0.86098\n",
      "  0.6705   -0.86556  -0.18234 ]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for a,b in word2vec.items() : \n",
    "    i+=1\n",
    "    if i==20 : break\n",
    "    print(f'word {a}   ,   with Vecs     {b[:10]}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.418     ,  0.24968   , -0.41242   ,  0.1217    ,  0.34527   ,\n",
       "       -0.044457  , -0.49688   , -0.17862   , -0.00066023, -0.6566    ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " ',',\n",
       " '.',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '-',\n",
       " 'that',\n",
       " 'on',\n",
       " 'is',\n",
       " 'was',\n",
       " 'said',\n",
       " 'with',\n",
       " 'he',\n",
       " 'as']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمليات الطرح"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man = queen - woman\n",
      "france - paris = britain - london\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "france - french = england - english\n",
      "japan - japanese = china - chinese\n",
      "japan - japanese = italy - italian\n",
      "japan - japanese = australia - australian\n",
      "december - november = july - june\n",
      "miami - florida = houston - texas\n",
      "einstein - scientist = matisse - painter\n",
      "china - rice = chinese - bread\n",
      "man - woman = he - she\n",
      "man - woman = uncle - aunt\n",
      "man - woman = brother - sister\n",
      "man - woman = friend - wife\n",
      "man - woman = actor - actress\n",
      "man - woman = father - mother\n",
      "heir - heiress = queen - princess\n",
      "nephew - niece = uncle - aunt\n",
      "france - paris = japan - tokyo\n",
      "france - paris = china - beijing\n",
      "february - january = october - november\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "cairo - egypt = damascus - syria\n"
     ]
    }
   ],
   "source": [
    "find_analogies('king', 'man', 'woman')\n",
    "find_analogies('france', 'paris', 'london')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "find_analogies('france', 'french', 'english')\n",
    "find_analogies('japan', 'japanese', 'chinese')\n",
    "find_analogies('japan', 'japanese', 'italian')\n",
    "find_analogies('japan', 'japanese', 'australian')\n",
    "find_analogies('december', 'november', 'june')\n",
    "find_analogies('miami', 'florida', 'texas')\n",
    "find_analogies('einstein', 'scientist', 'painter')\n",
    "find_analogies('china', 'rice', 'bread')\n",
    "find_analogies('man', 'woman', 'she')\n",
    "find_analogies('man', 'woman', 'aunt')\n",
    "find_analogies('man', 'woman', 'sister')\n",
    "find_analogies('man', 'woman', 'wife')\n",
    "find_analogies('man', 'woman', 'actress')\n",
    "find_analogies('man', 'woman', 'mother')\n",
    "find_analogies('heir', 'heiress', 'princess')\n",
    "find_analogies('nephew', 'niece', 'aunt')\n",
    "find_analogies('france', 'paris', 'tokyo')\n",
    "find_analogies('france', 'paris', 'beijing')\n",
    "find_analogies('february', 'january', 'november')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "find_analogies('cairo','egypt','syria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الكلمات الاقرب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors of: king\n",
      "\tprince\n",
      "\tqueen\n",
      "\tii\n",
      "\temperor\n",
      "\tson\n",
      "neighbors of: france\n",
      "\tfrench\n",
      "\tbelgium\n",
      "\tparis\n",
      "\tspain\n",
      "\tnetherlands\n",
      "neighbors of: japan\n",
      "\tjapanese\n",
      "\tchina\n",
      "\tkorea\n",
      "\ttokyo\n",
      "\ttaiwan\n",
      "neighbors of: einstein\n",
      "\trelativity\n",
      "\tbohr\n",
      "\tphysics\n",
      "\theisenberg\n",
      "\tfreud\n",
      "neighbors of: woman\n",
      "\tgirl\n",
      "\tman\n",
      "\tmother\n",
      "\ther\n",
      "\tboy\n",
      "neighbors of: nephew\n",
      "\tcousin\n",
      "\tbrother\n",
      "\tgrandson\n",
      "\tson\n",
      "\tuncle\n",
      "neighbors of: february\n",
      "\toctober\n",
      "\tdecember\n",
      "\tjanuary\n",
      "\taugust\n",
      "\tseptember\n",
      "neighbors of: rome\n",
      "\tnaples\n",
      "\tvenice\n",
      "\titaly\n",
      "\tturin\n",
      "\tpope\n",
      "neighbors of: bolt\n",
      "\tusain\n",
      "\tjavelin\n",
      "\tkitajima\n",
      "\thammer\n",
      "\trod\n",
      "neighbors of: bus\n",
      "\ttrain\n",
      "\tbuses\n",
      "\tpassenger\n",
      "\tcommuter\n",
      "\ttrains\n",
      "neighbors of: hello\n",
      "\tgoodbye\n",
      "\they\n",
      "\t!\n",
      "\tkiss\n",
      "\twow\n",
      "neighbors of: go\n",
      "\tgoing\n",
      "\tcome\n",
      "\tget\n",
      "\t'll\n",
      "\ttake\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors('king')\n",
    "nearest_neighbors('france')\n",
    "nearest_neighbors('japan')\n",
    "nearest_neighbors('einstein')\n",
    "nearest_neighbors('woman')\n",
    "nearest_neighbors('nephew')\n",
    "nearest_neighbors('february')\n",
    "nearest_neighbors('rome')\n",
    "\n",
    "nearest_neighbors('bolt')\n",
    "nearest_neighbors('bus')\n",
    "nearest_neighbors('hello')\n",
    "nearest_neighbors('go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "التجربة علي الملف الأكبر ذو 300 قيمة للتضمين"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تحميل الملف من هنا\n",
    "\n",
    "\n",
    "\n",
    "https://www.kaggle.com/datasets/thanakomsn/glove6b300dtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "embedding = []\n",
    "idx2word = []\n",
    "\n",
    "with open(r'D:\\Machine Learning\\NLP\\0 Data\\GloVe English\\glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        embedding.append(vec)\n",
    "        idx2word.append(word)\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "embedding = np.array(embedding)\n",
    "V, D = embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الاطلاع علي البيانات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word the   ,   with Vecs     [ 0.04656    0.21318   -0.0074364 -0.45854   -0.035639 ]\n",
      "word ,   ,   with Vecs     [-0.25539  -0.25723   0.13169  -0.042688  0.21817 ]\n",
      "word .   ,   with Vecs     [-0.12559   0.01363   0.10306  -0.10123   0.098128]\n",
      "word of   ,   with Vecs     [-0.076947 -0.021211  0.21271  -0.72232  -0.13988 ]\n",
      "word to   ,   with Vecs     [-0.25756  -0.057132 -0.6719   -0.38082  -0.36421 ]\n",
      "word and   ,   with Vecs     [ 0.038466 -0.039792  0.082747 -0.38923  -0.21431 ]\n",
      "word in   ,   with Vecs     [-0.44399  0.12817 -0.25247 -0.18582 -0.16614]\n",
      "word a   ,   with Vecs     [-0.29712   0.094049 -0.096662 -0.344    -0.18483 ]\n",
      "word \"   ,   with Vecs     [ 0.6947    0.22184   0.10526   0.012382 -0.2558  ]\n",
      "word 's   ,   with Vecs     [-0.001272  0.36514  -0.077363 -0.26559   0.17987 ]\n",
      "word for   ,   with Vecs     [-0.24132   0.12063   0.1919   -0.26692   0.061076]\n",
      "word -   ,   with Vecs     [ 0.040598 -0.028364  0.25009  -0.26526   0.43197 ]\n",
      "word that   ,   with Vecs     [-0.18256  0.49851 -0.1639  -0.17443 -0.16382]\n",
      "word on   ,   with Vecs     [ 0.12702  0.21062 -0.18983  0.17677 -0.11784]\n",
      "word is   ,   with Vecs     [-0.1749   0.22956  0.24924 -0.20512 -0.12294]\n",
      "word was   ,   with Vecs     [ 0.065573  0.022011 -0.13182  -0.2133   -0.045275]\n",
      "word said   ,   with Vecs     [-0.24135   0.15132   0.016839  0.15962   0.20608 ]\n",
      "word with   ,   with Vecs     [-0.50425  0.31073  0.1428  -0.49815 -0.48062]\n",
      "word he   ,   with Vecs     [-0.52057   0.49562   0.032912 -0.060807 -0.099912]\n",
      "word as   ,   with Vecs     [-0.056826  0.23863   0.44515  -0.014863  0.1713  ]\n",
      "word it   ,   with Vecs     [ 0.033284 -0.040754 -0.048377  0.12017  -0.13915 ]\n",
      "word by   ,   with Vecs     [ 0.073515  0.25736   0.2359    0.19299  -0.29611 ]\n",
      "word at   ,   with Vecs     [-0.36898   0.89703   0.34799  -0.062829  0.18046 ]\n",
      "word (   ,   with Vecs     [-0.1686  -0.1958  -0.20248  0.20137  0.28469]\n",
      "word )   ,   with Vecs     [-0.13877 -0.24946 -0.13896  0.105    0.2721 ]\n",
      "word from   ,   with Vecs     [-0.18559  -0.41772  -0.019296 -0.065099 -0.41371 ]\n",
      "word his   ,   with Vecs     [-0.2213    0.47139  -0.16658  -0.26128   0.040058]\n",
      "word ''   ,   with Vecs     [-0.32597  -0.3418    0.33624   0.017565  0.2899  ]\n",
      "word ``   ,   with Vecs     [-0.22276 -0.30464  0.17728 -0.10514  0.16184]\n",
      "word an   ,   with Vecs     [-0.3206    0.43316  -0.086867  0.3012   -0.093775]\n",
      "word be   ,   with Vecs     [-0.33848   0.42841  -0.10284  -0.22054   0.051708]\n",
      "word has   ,   with Vecs     [-0.2106    0.22677   0.063678  0.088348  0.2588  ]\n",
      "word are   ,   with Vecs     [-0.23589   0.3831    0.10834  -0.063118  0.031596]\n",
      "word have   ,   with Vecs     [-0.19563  0.48127 -0.2467   0.12751  0.30997]\n",
      "word but   ,   with Vecs     [-0.0093601  0.22789   -0.10275    0.0010893  0.21235  ]\n",
      "word were   ,   with Vecs     [-0.16909  0.15969 -0.18385 -0.14528 -0.17565]\n",
      "word not   ,   with Vecs     [ 0.0083903  0.28769   -0.23466   -0.26343   -0.05723  ]\n",
      "word this   ,   with Vecs     [-0.20437   0.16431   0.041794 -0.13708  -0.29779 ]\n",
      "word who   ,   with Vecs     [-0.52869   0.33011   0.18584   0.076365  0.23219 ]\n",
      "word they   ,   with Vecs     [-0.26625   0.47964  -0.38818   0.039296 -0.020338]\n",
      "word had   ,   with Vecs     [-0.32601     0.25829    -0.391      -0.13565    -0.00074383]\n",
      "word i   ,   with Vecs     [-0.13292   0.16985  -0.1436   -0.088722  0.07951 ]\n",
      "word which   ,   with Vecs     [-0.22232   0.23856  -0.048047 -0.25248  -0.08231 ]\n",
      "word will   ,   with Vecs     [-0.34572   0.28478  -0.48478  -0.34118   0.054563]\n",
      "word their   ,   with Vecs     [-0.070601  0.54909  -0.25536  -0.16544  -0.034464]\n",
      "word :   ,   with Vecs     [ 0.096348 -0.21428   0.14032  -0.054121  0.70784 ]\n",
      "word or   ,   with Vecs     [-0.26234   0.12633  -0.084185 -0.077551 -0.08509 ]\n",
      "word its   ,   with Vecs     [ 0.32509   0.01838  -0.071966 -0.42953  -0.14065 ]\n",
      "word one   ,   with Vecs     [-0.36756   0.395    -0.27034  -0.14817  -0.026378]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for a,b in word2vec.items() : \n",
    "    i+=1\n",
    "    if i==50 : break\n",
    "    print(f'word {a}   ,   with Vecs     {b[:5]}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04656  ,  0.21318  , -0.0074364, -0.45854  , -0.035639 ,\n",
       "        0.23643  , -0.28836  ,  0.21521  , -0.13486  , -1.6413   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " ',',\n",
       " '.',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '-',\n",
       " 'that',\n",
       " 'on',\n",
       " 'is',\n",
       " 'was',\n",
       " 'said',\n",
       " 'with',\n",
       " 'he',\n",
       " 'as']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمليات الطرح"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man = queen - woman\n",
      "france - paris = britain - london\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "france - french = england - english\n",
      "japan - japanese = china - chinese\n",
      "japan - japanese = italy - italian\n",
      "japan - japanese = australia - australian\n",
      "december - november = april - june\n",
      "miami - florida = dallas - texas\n",
      "einstein - scientist = picasso - painter\n",
      "china - rice = chinese - bread\n",
      "man - woman = he - she\n",
      "man - woman = uncle - aunt\n",
      "man - woman = brother - sister\n",
      "man - woman = brother - wife\n",
      "man - woman = actor - actress\n",
      "man - woman = father - mother\n",
      "heir - heiress = prince - princess\n",
      "nephew - niece = uncle - aunt\n",
      "france - paris = japan - tokyo\n",
      "france - paris = china - beijing\n",
      "february - january = october - november\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "cairo - egypt = damascus - syria\n"
     ]
    }
   ],
   "source": [
    "find_analogies('king', 'man', 'woman')\n",
    "find_analogies('france', 'paris', 'london')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "find_analogies('france', 'french', 'english')\n",
    "find_analogies('japan', 'japanese', 'chinese')\n",
    "find_analogies('japan', 'japanese', 'italian')\n",
    "find_analogies('japan', 'japanese', 'australian')\n",
    "find_analogies('december', 'november', 'june')\n",
    "find_analogies('miami', 'florida', 'texas')\n",
    "find_analogies('einstein', 'scientist', 'painter')\n",
    "find_analogies('china', 'rice', 'bread')\n",
    "find_analogies('man', 'woman', 'she')\n",
    "find_analogies('man', 'woman', 'aunt')\n",
    "find_analogies('man', 'woman', 'sister')\n",
    "find_analogies('man', 'woman', 'wife')\n",
    "find_analogies('man', 'woman', 'actress')\n",
    "find_analogies('man', 'woman', 'mother')\n",
    "find_analogies('heir', 'heiress', 'princess')\n",
    "find_analogies('nephew', 'niece', 'aunt')\n",
    "find_analogies('france', 'paris', 'tokyo')\n",
    "find_analogies('france', 'paris', 'beijing')\n",
    "find_analogies('february', 'january', 'november')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "\n",
    "find_analogies('cairo','egypt','syria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الكلمات الاقرب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors of: king\n",
      "\tqueen\n",
      "\tprince\n",
      "\tmonarch\n",
      "\tkingdom\n",
      "\tthrone\n",
      "neighbors of: france\n",
      "\tfrench\n",
      "\tparis\n",
      "\tbelgium\n",
      "\tspain\n",
      "\titaly\n",
      "neighbors of: japan\n",
      "\tjapanese\n",
      "\ttokyo\n",
      "\tkorea\n",
      "\tchina\n",
      "\tasia\n",
      "neighbors of: einstein\n",
      "\trelativity\n",
      "\tbohr\n",
      "\tphysicists\n",
      "\theisenberg\n",
      "\tsigmund\n",
      "neighbors of: woman\n",
      "\tgirl\n",
      "\tman\n",
      "\tmother\n",
      "\tshe\n",
      "\ther\n",
      "neighbors of: nephew\n",
      "\tbrother\n",
      "\tcousin\n",
      "\tgrandson\n",
      "\tuncle\n",
      "\tson\n",
      "neighbors of: february\n",
      "\toctober\n",
      "\tdecember\n",
      "\tjanuary\n",
      "\tnovember\n",
      "\tapril\n",
      "neighbors of: rome\n",
      "\titaly\n",
      "\tnaples\n",
      "\tturin\n",
      "\tvenice\n",
      "\troman\n",
      "neighbors of: bolt\n",
      "\tusain\n",
      "\tlocking\n",
      "\tcrossbow\n",
      "\tbolts\n",
      "\tasafa\n",
      "neighbors of: bus\n",
      "\tbuses\n",
      "\ttrain\n",
      "\tcommuter\n",
      "\ttaxi\n",
      "\ttruck\n",
      "neighbors of: hello\n",
      "\tgoodbye\n",
      "\they\n",
      "\t!\n",
      "\tdolly\n",
      "\tmuddah\n",
      "neighbors of: go\n",
      "\tgoing\n",
      "\tcome\n",
      "\tyou\n",
      "\tn't\n",
      "\t'll\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors('king')\n",
    "nearest_neighbors('france')\n",
    "nearest_neighbors('japan')\n",
    "nearest_neighbors('einstein')\n",
    "nearest_neighbors('woman')\n",
    "nearest_neighbors('nephew')\n",
    "nearest_neighbors('february')\n",
    "nearest_neighbors('rome')\n",
    "\n",
    "nearest_neighbors('bolt')\n",
    "nearest_neighbors('bus')\n",
    "nearest_neighbors('hello')\n",
    "nearest_neighbors('go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
